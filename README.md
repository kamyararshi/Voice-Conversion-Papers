# Voice COnversion Papers with Links

This repository contains an overview of voice conversion and some voice cloning research papers and the links to their paper and/or code repositories.

| Title | Conf. Name | Link | Description | Repo |
|-------|------------|------|-------------|------|
| DurFlex-EVC | nan | [Link](https://arxiv.org/abs/2401.08095) | Emotion Conversion of the voice | nan |
| DiffVC+ | nan | [Link](https://www.isca-archive.org/interspeech_2024/huang24_interspeech.pdf) | Ananymization using Gaussian sampled random style embeddings | nan |
| FastVoiceGrad | nan | [Link](http://www.arxiv.org/abs/2409.02245) | Faster Diffusion-based VC using one-step inference in reverse diffusion | nan |
| Vec-Tok-VC+ | nan | [Link](https://arxiv.org/abs/2406.09844) | Decomposing content and speaker features using RVQ and Clustering | nan |
| MulliVC | nan | [Link](https://arxiv.org/abs/2408.04708) | Cross Language VC in absence of parallel cross lingual data | nan |
| Diff-HierVC: Diffusion-based Hierarchical Voice Conversion with Robust Pitch Generation and Masked Prior for Zero-shot Speaker Adaptation. | Interspeech | [Link](https://arxiv.org/abs/2311.04693) | Voice2Voice: Two diffuison models DiffPitch and DiffVoice. DiffPitch for taget prompt and DiffVoice for final converted voice Dataset: VCTK | https://github.com/hayeong0/Diff-HierVC |
| Low-latency Real-time Voice Conversion on CPU | N.A. | [Link](https://koe.ai/papers/llvc.pdf) | Voice2Voice: One generator, light model  Dataset: LibriSpeech | https://github.com/KoeAI/LLVC |
| Phoneme Hallucinator: One-shot Voice Conversion via Set Expansion. | AAAI | [Link](https://arxiv.org/abs/2308.06382) | Voice2Voice: WavLM for source and target speech, Neighbour based VC pipeline  Dataset: LibriSpeech | https://github.com/PhonemeHallucinator/Phoneme_Hallucinator |
| End-to-End Zero-Shot Voice Conversion with Location-Variable Convolutions | Interspeech | [Link](https://arxiv.org/abs/2205.09784) | Voice2Voice:  ECAPA-TDNN for speaker embeddings. Content and speaker features from training utterances are used to reconstruct  Dataset: VCTK | https://github.com/wonjune-kang/lvc-vc |
| Voice Conversion With Just Nearest Neighbors. | Interspeech | [Link](https://arxiv.org/pdf/2305.18975.pdf) | Voice2Voice: Using WavLM and kNN regressor (self-supervised)  Dataset: LibriSpeech | https://github.com/bshall/knn-vc |
| Styletts-vc: One-shot voice conversion by knowledge transfer from style-based tts models. | IEEE Spoken Language Technology Workshop (SLT) | [Link](https://arxiv.org/abs/2212.14227) | Voice2Voice: Transfering the style of one speaker to another  Dataset: VCTK | https://github.com/yl4579/StyleTTS-VC |
| TriAAN-VC: Triple Adaptive Attention Normalization for Any-to-Any Voice Conversion. | ICASSP  | [Link](https://arxiv.org/abs/2303.09057) | Voice2Voice: comprises two encoders, extracting content and speaker information respectively, and a decoder. The encoders and decoder are connected via a bottleneck layer, similar approach to our work  Dataset: VCTK | https://github.com/winddori2002/TriAAN-VC |
| DISENTANGLED SPEECH REPRESENTATION LEARNING FOR ONE-SHOT CROSS-LINGUAL VOICE CONVERSION USING β-VAE | IEEE Spoken Language Technology Workshop (SLT) | [Link](https://www1.se.cuhk.edu.hk/~hccl/publications/pub/2023%20SLT2022-Beta_VAE_based_one_shot_cross_lingual_VC.pdf) | Voice2Voice:  one-shot cross-lingual voice conversion task to demonstrate the effectiveness of the disentanglement. Inspired by β-VAE Dataset: VCTK | https://github.com/light1726/BetaVAE_VC |
| FreeVC: Towards High-Quality Text-Free One-Shot Voice Conversion. | ICASSP  | [Link](https://arxiv.org/abs/2210.15418) | Voice2Voice: FreeVC contains a prior encoder, a posterior encoder, a decoder, a discriminator and a speaker encoder Dataset: VCTK | https://github.com/OlaWod/FreeVC |
| HiFi-VC: High Quality ASR-Based Voice Conversion. | Interspeech | [Link](https://arxiv.org/abs/2203.16937) | Voice2Voice: HiFi-Gan as vocoder, ASR based content encoder, speaker embedder similar to NVC-Net Dataset: VCTK | https://github.com/tinkoff-ai/hifi_vc |
| DeID-VC: Speaker De-identification via Zero-shot Pseudo Voice Conversion. | Interspeech | [Link](https://arxiv.org/abs/2209.04530) | Voice2Voice: a Variational Autoencoder (VAE) based Pseudo Speaker Generator (PSG) and a voice conversion Autoencoder (AE) under zero-shot settings Dataset: WSJ | https://github.com/a43992899/DeID-VC |
| Diffusion-based voice conversion with fast maximum likelihood sampling scheme. | ICLR | [Link](https://arxiv.org/pdf/2109.13821.pdf) | Voice2Voice: A one-shot many-to-many VC model. Introduces a novel DPM sampling scheme and establishes its connection with likelihood maximization Dataset: VCTK, LibriTTS | https://github.com/huawei-noah/Speech-Backbones/tree/main/DiffVC |
| Nvc-net: End-to-end adversarial voice conversion. | ICASSP  | [Link](https://arxiv.org/abs/2106.00992) | Voice2Voice: An ASR model is used to extract the linguistic features, speaker encoder extracts speaker embeddings Dataset: VCTK | https://github.com/sony/ai-research-code/tree/master/nvcnet |
| Any-to-many voice conversion with location-relative sequence-to-sequence modeling | IEEE/ACM Transactions on Audio, Speech, and Language Processing | [Link](https://arxiv.org/abs/2009.02725v3) | Voice2Voice: an encoder-decoder-based (CTC-attention) phoneme recognizer for content. Multi-speaker location-relative attention based seq2seq synthesis model for speaker identity  Dataset: VCTK | https://github.com/liusongxiang/ppg-vc |
| QuickVC: Any-to-many Voice Conversion Using Inverse Short-time Fourier Transform for Faster Conversion | IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) | [Link](https://arxiv.org/abs/2302.08296) | Voice2Voice: MS-iSTFT-Decoder vocoder, HuBERT encoder, Speaker encoder is one LSTM layer, relavant to our work Dataset: VCTK | https://github.com/quickvc/QuickVC-VoiceConversion |
| S2VC: A Framework for Any-to-Any Voice Conversion with Self-Supervised Pretrained Representations | Interspeech | [Link](https://arxiv.org/pdf/2104.02901v2.pdf) | Voice2Voice: The overall framework evolves from FragmentVC. Self-attention pooling guides the representation encoded by the source encoder to be close to that encoded by the target encoder. Dataset: VCTK | https://github.com/howard1337/S2VC |
| Assem-VC: Realistic Voice Conversion by Assembling Modern Speech Synthesis Techniques | ICASSP  | [Link](https://arxiv.org/pdf/2104.00931v2.pdf) | TextMel2Speech: Input source text and target mel generates converted mel and using fine-tuned HiFiGAN converted to voice | https://github.com/mindslab-ai/assem-vc |
| Speech Resynthesis from Discrete Disentangled Self-Supervised Representations | Interspeech | [Link](https://arxiv.org/pdf/2104.00355v3.pdf) | Voice2Voice: Hubert (Speech2unit)+Pitch2Unit+SPeaker EMbedder (Speaker ID pre-trained) + HiFiGAN Vocoder | https://github.com/facebookresearch/speech-resynthesis |
| Limited Data Emotional Voice Conversion Leveraging Text-to-Speech: Two-stage Sequence-to-Sequence Training | Interspeech | [Link](https://arxiv.org/pdf/2103.16809v2.pdf) | Text2Speech: 5 components, text enc, seq2seq ASR enc, style enc (emotion enc), classifier (emotion), se2seq decoder. Two staged training. | https://github.com/KunZhou9646/seq2seq-EVC |
| MaskCycleGAN-VC: Learning Non-parallel Voice Conversion with Filling in Frames | ICASSP  | [Link](https://arxiv.org/pdf/2102.12841v1.pdf) | Voice2Voice: Modified version of CycleGAN VC-2 trained using filling in frames (or in mel-spectrum) similar to image inpainting. | https://github.com/GANtastic3/MaskCycleGAN-VC |
| FastSVC: Fast Cross-Domain Singing Voice Conversion with Feature-wise Linear Modulation | ICME | [Link](https://arxiv.org/pdf/2011.05731v2.pdf) | Voice2Voice:Singing Voice Conversion, light weight, Conformer-based phoneme recognizer that extracts singer-agnostic linguistic features from singing signals. | https://github.com/liusongxiang/ppg-vc |
| FragmentVC: Any-to-Any Voice Conversion by End-to-End Extracting and Fusing Fine-Grained Voice Fragments With Attention | ICASSP  | [Link](https://arxiv.org/pdf/2010.14150v2.pdf) | Voice2Voice: Wav2Vec2 for content (source), log mel-spectogram for spectral features (target). Two-stged training for feature aligning based on Transformers. Reconstruction loss. | https://github.com/yistLin/FragmentVC |
| CycleGAN-VC3: Examining and Improving CycleGAN-VCs for Mel-spectrogram Conversion | Interspeech | [Link](https://arxiv.org/pdf/2010.11672v1.pdf) | Voice2Voice: Non-parallel VC challenge, CycleGAN-VC limitations, CycleGAN-VC3 with TFAN ( time-frequency adaptive normalization) enhancement, preserves time-frequency structure, outperforms CycleGAN-VC2 in naturalness and similarity. (see MaskCycleGAN-VC) | https://github.com/jackaduma/CycleGAN-VC3 |
| Towards Natural Bilingual and Code-Switched Speech Synthesis Based on Mix of Monolingual Recordings and Cross-Lingual Voice Conversion | Interspeech | [Link](https://arxiv.org/pdf/2010.08136v1.pdf) | Text2Voice: Aim to build a good bilingual speech synthesis system, with native like output. Based on Tacotron-2 and a Transformer-based synthesizer for augmentation to get bilingual data. | https://github.com/espnet/espnet |
| The Sequence-to-Sequence Baseline for the Voice Conversion Challenge 2020: Cascading ASR and TTS | Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge  | [Link](https://arxiv.org/pdf/2010.02434v1.pdf) | Voice2Voice: A naive approach converting voice to text using ASR followed by a TTS model plus a Neural Vocoder on top. | https://github.com/espnet/espnet |
| Nonparallel Voice Conversion with Augmented Classifier Star Generative Adversarial Networks | IEEE/ACM Transactions on Audio, Speech, and Language Processing | [Link](https://arxiv.org/pdf/2008.12604v7.pdf) | Mel2Mel: StarGAN-based proposing "augmented classifier StarGAN" that does not require any info about the domain of the speech at test time. USing a single generator, it learns the mapping among multiple speech domains. | https://github.com/kamepong/StarGAN-VC |
| VQVC+: One-Shot Voice Conversion by Vector Quantization and U-Net architecture | Interspeech | [Link](https://arxiv.org/pdf/2006.04154v1.pdf) | Voice2Voice: Feature disentanglement using U-Net based neural network and vector quantization.(Using skip connections and quantized connections) | https://github.com/ericwudayi/SkipVQVC |
| StarGAN-VC: Non-parallel many-to-many voice conversion with star generative adversarial networks | IEEE Spoken Language Technology Workshop (SLT) | [Link](https://arxiv.org/pdf/1806.02169v2.pdf) | Voice2Voice: While CycleGAN-VC allows the generation of naturalsounding speech when a sufficient number of training examples are available, one limitation is that it only learns one-to-one-mappings. Here, we propose using StarGAN [42] to develop a method that allows non-parallel many-to-many VC. We call the present method StarGAN-VC. (Highly cited) | https://github.com/kamepong/StarGAN-VC |
| AUTOVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss | International Conference on Machine Learning | [Link](https://arxiv.org/pdf/1905.05879v2.pdf) | Voice2Voice: Autoencoder-based style transfer using two encoders for source, and target and a decoder for generation. | https://github.com/liusongxiang/StarGAN-Voice-Conversion |
| One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization | Interspeech | [Link](https://arxiv.org/pdf/1904.05742v4.pdf) | Voice2Voice: Disentangles speaker and content representations using Instance Normalization (IN). (Claim) This allows their model to do VC for unseen speakers during the training. They use, Speaker encoder, Content encoder (with IN),  and a decoder (with AdaIN) to do the VC | https://github.com/jjery2243542/adaptive_voice_conversion |
| DurFlex-EVC: Duration-Flexible Emotional Voice Conversion with Parallel Generation | N.A. | [Link](https://arxiv.org/pdf/2401.08095v1.pdf) | Voice2Voice: By intergrating a style auto-encoder and unit aligner, and training using parallel dataset of ESD (for emotion) they calim more control over Emotional VC. Style autoencoder is used for disentanglement and manipulation of style elements. (Pretty complicated stuff here) | https://github.com/hs-oh-prml/durflexevc |
| Emo-StarGAN: A Semi-Supervised Any-to-Many Non-Parallel Emotion-Preserving Voice Conversion | Interspeech | [Link](https://arxiv.org/pdf/2309.07586v1.pdf) | Voice2Voice: A modifed STarGAN-VC version trained using emotion labelled and non-parallel datasets plus integrating emotion loss componnents to better preserve emotion. (Basically used StarGAN-VC while adding some loss terms) | https://github.com/suhitaghosh10/emo-stargan |
| DuTa-VC: A Duration-aware Typical-to-atypical Voice Conversion Approach with Diffusion Probabilistic Model | Interspeech | [Link](https://arxiv.org/pdf/2306.10588v1.pdf) | Voice2Voice: Encoder (phoneme and phoneme duration predictor) for source mel, Decoder for reverse diffusion (Diffusion probab. models) leading to generating target mel, and a Vocoder to recosntruct waveform. | https://github.com/wanghelin1997/duta-vc |
| ControlVC: Zero-Shot Voice Conversion with Time-Varying Controls on Pitch and Speed | Interspeech | [Link](https://arxiv.org/pdf/2209.11866v5.pdf) | Voice2Voice: Time varying control on pitch and speed. Modifying the speed of the source utterance using TD-PSOLA pre-processing, pitch control by modifying the pitch contour of the speed-controlled source utterance, and uses a VQ-VAE pitch encoder to compute discrete pitch embedding, HuBERT extracts the linguistic embedding, and A modified version of the HiFi-GAN vocoder to generate waveform. | https://github.com/MelissaChen15/control-vc |
| End-to-End Zero-Shot Voice Conversion with Location-Variable Convolutions | Interspeech | [Link](https://arxiv.org/pdf/2205.09784v3.pdf) | Voice2Voice: (MIT) They have employed a kernel predictor for LVC inside their Generator which consists of multiple LVC blocks and do VC by using F0 extraction for content info.They claim this is the first end2end pipeline with good quality and zero-shot capability | https://github.com/wonjune-kang/lvc-vc |
| Towards Improved Zero-shot Voice Conversion with Conditional DSVAE | Interspeech | [Link](https://arxiv.org/pdf/2205.05227v2.pdf) | Voice2Voice: Conditioning on content bias using proposed disentangled sequential variational autoencoder (DSVAE).  Trained on VCTK | https://github.com/jlian2/Improved-Voice-Conversion-with-Conditional-DSVAE |
| An Overview of Voice Conversion and Its Challenges: From Statistical Modeling to Deep Learning | nan | [Link](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9262021&tag=1) | Overview from statitical models to DL methods | nan |
| SELFVC: VOICE CONVERSION WITH ITERATIVE REFINEMENT USING SELF TRANSFORMATIONS | nan | [Link](https://arxiv.org/pdf/2310.09653.pdf) | N/A | nan |
| SpeechSplit 2.0: Unsupervised speech disentanglement for voice conversion Without tuning autoencoder Bottlenecks | ICASSP  | [Link](https://arxiv.org/pdf/2203.14156v1.pdf) | Voice2Voice: Speech component disentangled at the autoencoder input using efficient signal processing techniques instead of relying on bottleneck tuning. | https://github.com/biggytruck/speechsplit2 |
| YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone | ICML | [Link](https://arxiv.org/pdf/2112.02418v4.pdf) | Text2Voice: multilingual transformer-based text encoder, stochastic duration prediction and speaker consistency loss during training for improved synthesis quality and speaker similarity. HiFi-GAN for waveform generation. Inference: the model utilizes Monotonic Alignment Search for generating human-like rhythms of speech | https://github.com/coqui-ai/TTS |
| SIG-VC: A Speaker Information Guided Zero-shot Voice Conversion System for Both Human Beings and Machines | ICASSP  | [Link](https://arxiv.org/pdf/2111.03811v3.pdf) | Voice2Voice: Supervise the intermediate representation to remove the speaker information from the linguistic information, pre-trained acoustic model used to extract the linguistic feature | https://github.com/HaydenCaffrey/SIG-VC |
| S3PRL-VC: Open-source Voice Conversion Framework with Self-supervised Speech Representations | ICASSP  | [Link](https://arxiv.org/pdf/2110.06280v1.pdf) | Voice2Voice: The source speech are first extracted by a recognizer, then synthesized into the converted speech by a synthesizer, then speech representations is learned from large-scale unlabeled data | https://github.com/s3prl/s3prl |
| StarGANv2-VC: A Diverse, Unsupervised, Non-parallel Framework for Natural-Sounding Voice Conversion | Interspeech | [Link](https://arxiv.org/pdf/2107.10394v2.pdf) | VoiceMel2Speech: generator converting mel-spectrograms into frequency representations, a mapping network for diverse style generation, a style encoder for reference-based style extraction, and discriminators to capture domain-specific features, including an additional classifier for feedback on domain-specific characteristics to improve sample similarityM | https://github.com/yl4579/StarGANv2-VC |
| VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-shot Voice Conversion | Interspeech | [Link](https://arxiv.org/pdf/2106.10132v1.pdf) | VoiceMel2Voice: Content encoder, speaker encoder, pitch extractor, and decoder for unsupervised voice conversion. It utilizes variational approximation networks to minimize mutual information (MI) between content, speaker, and pitch representations, achieving disentanglement without text or speaker labels. One-shot voice conversion is achieved by extracting representations from the source and target speakers, then using the decoder to generate converted mel-spectrograms. | https://github.com/Wendison/VQMIVC |
